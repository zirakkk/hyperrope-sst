# HyperRoPE-SST ğŸ›°ï¸ğŸ”¬

<p align="center">
<a href="https://doi.org/10.1109/JSTARS.2025.3643365"><img src="https://img.shields.io/badge/Paper-IEEE%20JSTARS%202025-blue" alt="Paper"></a>
<a href="https://github.com/zirakkk/hyperrope-sst"><img src="https://img.shields.io/badge/Code-GitHub-black" alt="Code"></a>
<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey" alt="License"></a>
</p>

<h3 align="center">Spatial-Spectral Transformer with Patch-Local Mixed-Axis 2D Rotary Position Embedding for Hyperspectral Image Classification [IEEE JSTARS 2025 ğŸ”¥]</h3>

<h4 align="center"><a href="https://scholar.google.com/citations?user=T4oy5R0AAAAJ">Zirak Khan</a>, Noyon Dey, Kavya Kathiravan, Seung-Chul Yoon, Suchendra Bhandarkar</h4>

<h4 align="center"><strong>University of Georgia & U.S. Department of Agriculture</strong></h4>

---

#### **Performance Benchmarks**

[![Indian Pines](https://img.shields.io/badge/Indian%20Pines-96.81%25-blue)](#)
[![Pavia University](https://img.shields.io/badge/Pavia%20University-95.65%25-purple)](#)
[![Houston 2013](https://img.shields.io/badge/Houston%202013-93.38%25-orange)](#)
[![Kennedy Space Center](https://img.shields.io/badge/Kennedy%20Space%20Center-99.62%25%20-brightgreen)](#)
[![Salinas](https://img.shields.io/badge/Salinas-95.59%25-red)](#)

---

## ğŸ“¢ Latest Updates

- **Dec-04-25**: HyperRoPE-SST accepted at **IEEE JSTARS**! ğŸ”¥ğŸ¥³
- **Dec-01-25**: Open-sourced implementation with 8 baseline models for comprehensive comparison ğŸ”¥

---

## HyperRoPE-SST Overview ğŸ’¡

**HyperRoPE-SST** is a transformer-based architecture that introduces **patch-local mixed-axis 2D Rotary Position Embedding (RoPE) with learnable frequencies** for hyperspectral image classification. Unlike traditional absolute or bias-based positional embeddings that fail to preserve geometric relationships, our approach maintains precise diagonal and multi-directional spatial-spectral interactions between pixels within hyperspectral patches while adapting to dataset-specific characteristics.

The pipeline employs **PCA for spectral dimensionality reduction**, extracts **target-centered local patches**, embeds them using **shallow convolutions**, and applies a **2D RoPE-enhanced transformer encoder** with **center-focused attention** to produce context-aware target pixel embeddings. HyperRoPE-SST consistently outperforms state-of-the-art CNN and transformer-based models across five benchmark datasets, demonstrating superior accuracy for land-cover classification, environmental monitoring, and remote sensing applications.

---

## Key Features ğŸ†

- **Patch-Local Mixed-Axis 2D RoPE**: Novel learnable frequency-based rotary position embedding that preserves geometric relationships and captures diagonal/multi-directional spatial-spectral interactions
- **Dataset-Adaptive Learning**: Learnable frequencies adapt to dataset-specific spatial-spectral characteristics during training
- **Center-Focused Attention**: Weighted aggregation of center pixel representations across transformer layers for precise target pixel classification
- **State-of-the-Art Performance**: Achieves 96.81%, 95.65%, 93.38%, 99.62%, and 95.59% OA on IP, PU, HU, KSC, and SAL respectively
- **Comprehensive Baseline Comparison**: Includes 8 state-of-the-art models (CNN-based, ViT-based and hybrid architectures)

---

## Architecture âš™ï¸

### Overall Framework

<p align="center">
  <img src="classification_maps/assets/HyperRoPE-SST_Diagram.png" alt="HyperRoPE-SST Architecture" width="100%">
  <br>
  <em>Overall architecture of HyperRoPE-SST showing the complete pipeline from hyperspectral patch to classification </p>

### Patch-local Mixed-Axis 2D RoPE

<p align="center">
  <img src="classification_maps/assets/RoPE2D.png" alt="HyperRoPE-SST Architecture" width="100%">
  <br>
  <em> Comparison between (a) Xâ€“Y axis 2D RoPE and (b) the proposed patch-local mixed-axis 2D RoPE. Left: geometric rotation interpretation for a
sample center-pixel query. Right: attention heatmaps for a single query token over all keys in the hyperspectral patch. The proposed mixed-axis formulation
captures diagonal and off-axis spatial-spectral relationships more accurately and produces smoother, radially consistent attention patterns`</em>`</p>


---


## Directory Structure ğŸ“

```
hyperrope-sst/
â”œâ”€â”€ configs/                         # Model configurations
â”‚   â”œâ”€â”€ config.py              
â”‚   â”œâ”€â”€ hyper2Drope.json             # Hyper2DRoPE configuration
â”‚   â”œâ”€â”€ lsga_vit.json                # LSGA-ViT configuration
â”‚   â”œâ”€â”€ hit.json                     # HiT configuration
â”‚   â”œâ”€â”€ spectralformer.json          # SpectralFormer configuration
â”‚   â”œâ”€â”€ sqsformer.json               # SQSFormer configuration
â”‚   â”œâ”€â”€ conv2d.json                  # Conv2D configuration
â”‚   â”œâ”€â”€ conv3d.json                  # Conv3D configuration
â”‚   â””â”€â”€ ssrn.json                    # SSRN configuration
â”œâ”€â”€ data/                  
â”‚   â”œâ”€â”€ data_loader.py               # Multi-file HSI data loader
â”‚   â””â”€â”€ dataset/                     # Dataset directory
â”‚       â”œâ”€â”€ Houston/                 # Houston dataset (.mat files)
â”‚       â”œâ”€â”€ IndianPine/              # Indian Pines dataset
â”‚       â”œâ”€â”€ KSC/                     # Kennedy Space Center dataset
â”‚       â”œâ”€â”€ Pavia/                   # Pavia University dataset
â”‚       â””â”€â”€ Salinas/                 # Salinas dataset
â”œâ”€â”€ models/                          # Model implementations
â”‚   â”œâ”€â”€ hyperrope_vit.py             # Hyper2DRoPE (Ours)
â”‚   â”œâ”€â”€ lsga_vit.py                  # LSGA-ViT
â”‚   â”œâ”€â”€ hit.py                       # HiT
â”‚   â”œâ”€â”€ spectralformer.py            # SpectralFormer
â”‚   â”œâ”€â”€ sqsformer.py                 # SQSFormer
â”‚   â”œâ”€â”€ conv2d.py                    # 2D CNN baseline
â”‚   â”œâ”€â”€ conv3d.py                    # 3D CNN baseline
â”‚   â””â”€â”€ ssrn.py                      # SSRN
â”œâ”€â”€ utils/                   
â”‚   â”œâ”€â”€ trainer.py                   # Training logic for all models
â”‚   â”œâ”€â”€ evaluation.py                # Evaluation metrics (OA, AA, Kappa)
â”‚   â”œâ”€â”€ visualization.py             # Classification map visualization
â”‚   â””â”€â”€ utils.py                     # Helper functions
â”œâ”€â”€ checkpoints/                     # Best model checkpoints (8 models Ã— 5 datasets = 40 .pth files)
â”œâ”€â”€ classification_maps/             # Generated classification maps
â”‚   â”œâ”€â”€ Houston/
â”‚   â”œâ”€â”€ IndianPine/
â”‚   â”œâ”€â”€ KSC/
â”‚   â”œâ”€â”€ Pavia/
â”‚   â””â”€â”€ Salinas/
â”œâ”€â”€ results/                         # Evaluation results (JSON)
â”‚   â”œâ”€â”€ 1) IndianPine Best Models Results/
â”‚   â”œâ”€â”€ 2) Pavia Best Models Results/
â”‚   â”œâ”€â”€ 3) Houston Best Models Results/
â”‚   â”œâ”€â”€ 4) KSC Best Models Results/
â”‚   â””â”€â”€ 5) Salinas Best Model Results/
â”œâ”€â”€ environment.yml                  # Conda environment specification
â”œâ”€â”€ main.py                          # Unified entry point
â””â”€â”€ README.md                        # This file
```

---

## Installation

### Prerequisites

- Python 3.10+
- CUDA 11.8+ (for GPU acceleration)
- Conda package manager
- 8GB+ GPU memory recommended

### Setup ğŸ”§

We recommend setting up a conda environment for the project:

```bash
# Create and activate environment
conda env create -f environment.yml
conda activate plasticseg

# Clone repository
git clone https://github.com/zirakkk/hyperrope-sst.git
cd hyperrope-sst
```

### Download Datasets ğŸ“¥

Download the five benchmark hyperspectral datasets from the [GIC Hyperspectral Remote Sensing Scenes](http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes) repository or IEEE GRSS Data Fusion Contest archives. Place the `.mat` files in their respective subdirectories: `IndianPine.mat` â†’ `data/dataset/IndianPine/`, `Pavia.mat` â†’ `data/dataset/Pavia/`, `Houston.mat` â†’ `data/dataset/Houston/`, `KSC.mat` â†’ `data/dataset/KSC/`, and `Salinas.mat` â†’ `data/dataset/Salinas/`. And split the data into TR, VA and TE using `generate_splits.py` which replicates spectralformer paper splits.

For a complete list of dependencies, see [`environment.yml`](environment.yml).

---

## Quick Start Examples ğŸš€

### Training & Testing HyperRoPE-SST (Our Method)

```bash
# Train HyperRoPE-SST on Indian Pines dataset
python main.py --config hyper2Drope.json --dataset IndianPine --model Hyper2DRoPE

# Train on other datasets (Pavia, Houston, KSC, Salinas)
python main.py --config hyper2Drope.json --dataset Pavia --model Hyper2DRoPE
python main.py --config hyper2Drope.json --dataset Houston --model Hyper2DRoPE
python main.py --config hyper2Drope.json --dataset KSC --model Hyper2DRoPE
python main.py --config hyper2Drope.json --dataset Salinas --model Hyper2DRoPE
```

### Training Baseline Models

The repository supports 7 baseline models with the same command structure:

```bash
# Example: Train LSGA-ViT on Salinas dataset
python main.py --config lsga_vit.json --dataset Salinas --model LSGA_ViT

# Other models: HiT, SpectralFormer, SQSFormer, Conv2d, Conv3d, SSRN
# Replace --config [model].json, --dataset [DatasetName], --model [ModelName]
```

**Available Options:**

- `--config`: `hyper2Drope.json`, `lsga_vit.json`, `hit.json`, `spectralformer.json`, `sqsformer.json`, `conv2d.json`, `conv3d.json`, `ssrn.json`
- `--dataset`: `IndianPine`, `Pavia`, `Houston`, `KSC`, `Salinas`
- `--model`: `Hyper2DRoPE`, `LSGA_ViT`, `HiT`, `SpectralFormer`, `SQSFormer`, `Conv2d`, `Conv3d`, `SSRN`

---

## Configuration âš™ï¸

Each model has a dedicated JSON configuration file in the `configs/` directory. Key parameters include:

### Network Architecture Parameters

```json
{
  "net": {
    "trainer": "hyperrope_vit",
    "pos_encoding_type": "rope_2d_mixed",     // Options: "rope_2d_axial", "absolute"
    "depth": 3,                             // Number of transformer layers
    "dim": 64,                              // Embedding dimension
    "heads": 8,                             // Number of attention heads
    "mlp_intermediate_dim": 256,              // MLP hidden dimension
    "kernal": 3,                             // Conv kernel size
    "padding": 1                            // Conv padding
  }
}
```

### Training Parameters

```json
{
  "train": {
    "epochs": 200,                          // Maximum training epochs
    "patience": 100,                        // Early stopping patience
    "lr": 0.001,                          
    "weight_decay": 0                   
  }
}
```

### Dataset-Specific Parameters

```json
{
  "datasets": {
    "Salinas": {
      "data_sign": "Salinas",
      "num_classes": 16,                    // Number of classes
      "patch_size": 21,                      // Spatial patch size
      "pca": 11,                            // PCA components (0 = no PCA)
      "spectral_size": 204,                   // Original spectral bands
      "batch_size": 40,
      "padding": true                        // Whether to pad image borders
    }
  }
}
```

---

## Results ğŸ“Š

### Quantitative Results

**HyperRoPE-SST** achieves state-of-the-art performance across all five benchmark datasets, demonstrating the effectiveness of patch-local mixed-axis 2D RoPE with learnable frequencies:

| **Dataset** | **OA (%)** | **AA (%)** | **Kappa (Îº)** |
| :---------------: | ---------------- | ---------------- | -------------------- |
|   Indian Pines   | **96.81**  | **95.73**  | **0.9634**     |
| Pavia University | **95.65**  | **94.82**  | **0.9420**     |
|   Houston 2013   | **93.38**  | **93.01**  | **0.9271**     |
|        KSC        | **99.62**  | **99.54**  | **0.9957**     |
|      Salinas      | **95.59**  | **97.12**  | **0.9509**     |

HyperRoPE-SST consistently outperforms CNN-based models (Conv2D, Conv3D, SSRN) and transformer-based methods (LSGA-ViT, HiT, SpectralFormer, SQSFormer) across all metrics. Detailed per-class accuracies available in paper.

### Qualitative Results

All predicted maps are available in `classification_maps/` with ground truth references and model predictions for qualitative comparison.

<p align="center">
  <img src="classification_maps/Pavia/Pavia_combined_classification_maps.png" alt="HyperRoPE-SST Architecture" width="100%">
  <br>
  <em> Predicted classification maps of the Pavia University dataset: (a) Ground truth, (b) 2D CNN (OA = 86.12%), (c) 3D CNN (OA = 83.92%), (d) SSRN (OA =
87.09%), (e) SF (OA = 82.35%), (f) HiT (OA = 79.73%), (g) LSGA (OA = 83.92%), (h) SQS (OA = 92.27%), (i) Ours (OA = 95.65% </p>


---

## Pretrained Models ğŸ’¾

The `checkpoints/` directory stores the best-performing model weights for each model-dataset combination (8 models Ã— 5 datasets = 40 `.pth` files). Each checkpoint is saved after achieving the highest validation accuracy during training and can be used for inference or fine-tuning on related tasks.

---

## Acknowledgements ğŸ™

- **IEEE GRSS** for providing benchmark hyperspectral datasets
- **Remote Sensing Community** for open-source models

This work was supported by the U.S. Department of Agriculture-Agricultural Research Service (USDA-ARS). Thanks to Dr. Seung-Chul Yoon for his unwavering support throughout the project.Special thanks to all co-authors for their valuable contributions and insights throughout the project.

---

## Citation ğŸ“œ

If you use HyperRoPE-SST in your research, please cite our paper:

```bibtex
@article{khan2025hyperrope,
  title={Spatial-Spectral Transformer with Patch-Local Mixed-Axis 2D
  Rotary Position Embedding for Hyperspectral Image Classification},
  author={Khan, Zirak and Dey, Noyon and Kathiravan, Kavya and Yoon, 
  Seung-Chul and Bhandarkar, Suchendra M.},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations
  and Remote Sensing},
  year={2025},
  doi={10.1109/JSTARS.2025.3643365}
}
```

---

## Contact âœ‰ï¸

For questions, collaborations, or support:

- ğŸ“§ **Email**: Zirak.khan@uga.edu
- ğŸ› **Issues**: [GitHub Issues](https://github.com/ZirakKhan/hyperrope-sst/issues)

---

## License

This project is licensed under the [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).

---

Looking forward to your feedback, contributions, and stars! ğŸŒŸ

<p align="center">
    <img src="https://badgen.net/github/stars/zirakkk/hyperrope-sst" alt="GitHub stars">
    <img src="https://badgen.net/github/forks/zirakkk/hyperrope-sst" alt="GitHub forks">
    <img src="https://badgen.net/github/watchers/zirakkk/hyperrope-sst" alt="GitHub watchers">
</p>
